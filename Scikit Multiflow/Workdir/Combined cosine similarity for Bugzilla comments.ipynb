{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity comparison based on TF-IDF and Word2Vec\n",
    "This experiment is based on my summer research report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I -- Cosine similarity calculation before and after change points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset and convert it into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from skmultiflow.drift_detection import ANGLE\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "df = pd.read_csv(\"OF.csv\")\n",
    "dataset = df[[\"timestamp\", \"bug\"]]\n",
    "\n",
    "bug_dic = {}\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    bug_no = row[\"bug\"]\n",
    "    if bug_no not in bug_dic:\n",
    "        bug_dic[bug_no] = [row[\"timestamp\"]]\n",
    "    elif bug_no in bug_dic:\n",
    "        bug_dic[bug_no].append(row[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate hour-based stream for each bug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generation(time_list, start, end):\n",
    "    days = (end - start) // 3600\n",
    "    stream = [0] * days\n",
    "    \n",
    "    for time in time_list:\n",
    "        index = (time - start) // 3600\n",
    "        stream[index] += 1\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process each bugs and generate change points reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1169"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_info = {}\n",
    "for key in bug_dic:\n",
    "    bug_list = bug_dic[key]\n",
    "    if len(bug_list) >= 100:\n",
    "        bug_list.sort()\n",
    "        start_time = int(datetime.datetime.fromtimestamp(bug_list[0] // 86400 * 86400).timestamp())\n",
    "        end_time = int(datetime.datetime.fromtimestamp((bug_list[-1] // 86400 + 1) * 86400).timestamp())\n",
    "        data_stream = stream_generation(bug_list, start_time, end_time)\n",
    "        index = 0\n",
    "        angle = ANGLE()\n",
    "        change_info[key] = [0]\n",
    "        for data in data_stream:\n",
    "            index += 1\n",
    "            angle.add_element(data)\n",
    "            if angle.detected_change():\n",
    "                true_point = index - angle.get_drift_location()\n",
    "                time_diff = index - true_point\n",
    "                if time_diff > 0:\n",
    "                    true_point = index - angle.get_drift_location()\n",
    "                    change_info[key].append(true_point)\n",
    "        change_info[key].append(index)\n",
    "len(change_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import bug comments dataset and convert it into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bug</th>\n",
       "      <th>Comment_author</th>\n",
       "      <th>comment_type</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Comment_content</th>\n",
       "      <th>Processed_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332722</td>\n",
       "      <td>Bob Clary [:bc:]</td>\n",
       "      <td>\\nDescription\\n</td>\n",
       "      <td>1144174304</td>\n",
       "      <td>My test run on 2006-04-01 showed a number of d...</td>\n",
       "      <td>test run showed number date related errors bel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332722</td>\n",
       "      <td>Bob Clary [:bc:]</td>\n",
       "      <td>\\nComment 2\\n</td>\n",
       "      <td>1144619799</td>\n",
       "      <td>I am unable to reproduce (so far) the other cr...</td>\n",
       "      <td>unable reproduce far crashes ecma date js ecma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332722</td>\n",
       "      <td>Bob Clary [:bc:]</td>\n",
       "      <td>\\nComment 3\\n</td>\n",
       "      <td>1145337493</td>\n",
       "      <td>Filed bug 334427 on the ecma_3/Date/15.9.5.5.j...</td>\n",
       "      <td>filed bug ecma date js ecma date js windows js...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332722</td>\n",
       "      <td>Bob Clary [:bc:]</td>\n",
       "      <td>\\nComment 4\\n</td>\n",
       "      <td>1145640762</td>\n",
       "      <td>This test appears to have problems around midn...</td>\n",
       "      <td>test appears problems around midnight test ecm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>332722</td>\n",
       "      <td>Bob Clary [:bc:]</td>\n",
       "      <td>\\nComment 5\\n</td>\n",
       "      <td>1257165472</td>\n",
       "      <td>re-enable ecma_3/Date/15.9.5.5.js which only f...</td>\n",
       "      <td>enable ecma date js fails linux dst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bug    Comment_author     comment_type   timestamp  \\\n",
       "0  332722  Bob Clary [:bc:]  \\nDescription\\n  1144174304   \n",
       "1  332722  Bob Clary [:bc:]    \\nComment 2\\n  1144619799   \n",
       "2  332722  Bob Clary [:bc:]    \\nComment 3\\n  1145337493   \n",
       "3  332722  Bob Clary [:bc:]    \\nComment 4\\n  1145640762   \n",
       "4  332722  Bob Clary [:bc:]    \\nComment 5\\n  1257165472   \n",
       "\n",
       "                                     Comment_content  \\\n",
       "0  My test run on 2006-04-01 showed a number of d...   \n",
       "1  I am unable to reproduce (so far) the other cr...   \n",
       "2  Filed bug 334427 on the ecma_3/Date/15.9.5.5.j...   \n",
       "3  This test appears to have problems around midn...   \n",
       "4  re-enable ecma_3/Date/15.9.5.5.js which only f...   \n",
       "\n",
       "                                   Processed_comment  \n",
       "0  test run showed number date related errors bel...  \n",
       "1  unable reproduce far crashes ecma date js ecma...  \n",
       "2  filed bug ecma date js ecma date js windows js...  \n",
       "3  test appears problems around midnight test ecm...  \n",
       "4                enable ecma date js fails linux dst  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"intermittent_bug_comments.csv\", engine='python')\n",
    "\n",
    "bugs_dict = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    bug_no = row[\"Bug\"]\n",
    "    try:\n",
    "        if bug_no not in bugs_dict:\n",
    "            bugs_dict[bug_no] = [[row[\"timestamp\"]]+row[\"Processed_comment\"].split()]\n",
    "        elif bug_no in bugs_dict:\n",
    "            bugs_dict[bug_no].append([row['timestamp']]+row[\"Processed_comment\"].split())\n",
    "    except:\n",
    "        continue\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the dictionary to get selected bug comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = []\n",
    "for key in bugs_dict:\n",
    "    if key not in change_info:\n",
    "        keys.append(key)\n",
    "        continue\n",
    "    if len(change_info[key]) == 2:\n",
    "        keys.append(key)\n",
    "for key in keys:\n",
    "    del bugs_dict[key]\n",
    "len(bugs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import several online and our own pre-training Word2Vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15475279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mr.nothing/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pretrain_model = api.load('word2vec-google-news-300')\n",
    "print(pretrain_model.wv.similarity(\"python\", \"js\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31644663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mr.nothing/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pretrain_model2 = api.load('fasttext-wiki-news-subwords-300')\n",
    "print(pretrain_model2.wv.similarity(\"python\", \"js\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.030526154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mr.nothing/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pretrain_model3 = api.load('glove-wiki-gigaword-300')\n",
    "print(pretrain_model3.wv.similarity(\"python\", \"js\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.02326997\n"
     ]
    }
   ],
   "source": [
    "pretrain_model4 = Word2Vec.load('bug_comments_model.bin')\n",
    "print(pretrain_model4.wv.similarity(\"python\", \"js\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and Word2Vec combined cosine similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def cosine_sim(list1, list2, doc1, doc2, model):\n",
    "    extra = []\n",
    "    vec1 = np.zeros((100, ), dtype='float32')\n",
    "    counter1 = 0\n",
    "    for sub in list1:\n",
    "        for word in sub:\n",
    "            try:\n",
    "                vec1 = np.add(vec1, model[word])\n",
    "                counter1 += 1\n",
    "            except:\n",
    "                extra.append(word)\n",
    "                continue\n",
    "    try:\n",
    "        vec1 = np.divide(vec1, counter1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    vec2 = np.zeros((100, ), dtype='float32')\n",
    "    counter2 = 0\n",
    "    for sub in list2:\n",
    "        for word in sub:\n",
    "            try:\n",
    "                vec2 = np.add(vec2, model[word])\n",
    "                counter2 += 1\n",
    "            except:\n",
    "                extra.append(word)\n",
    "                continue\n",
    "    try:\n",
    "        vec2 = np.divide(vec2, counter2)\n",
    "    except:pass\n",
    "\n",
    "    extra = {i:extra.count(i) for i in set(extra)}\n",
    "    word2vec_sim = 1 - spatial.distance.cosine(vec1, vec2)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([doc1, doc2])\n",
    "    denselist = vectors.todense().tolist()\n",
    "    tfidf_sim = 1 - spatial.distance.cosine(denselist[0], denselist[1])\n",
    "    \n",
    "    cos_sim = word2vec_sim + tfidf_sim\n",
    "    \n",
    "    return cos_sim, extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help function: generate space split document from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_generate(words):\n",
    "    doc = []\n",
    "    for sub in words:\n",
    "        for word in sub:\n",
    "            doc.append(word)\n",
    "    return \" \".join(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process two datasets and pick out the expected change points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mr.nothing/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n",
      "/Users/mr.nothing/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23618090452261306\n"
     ]
    }
   ],
   "source": [
    "report_info = {}\n",
    "bugs = [key for key in bugs_dict.keys()]\n",
    "total = 0\n",
    "counter = 0\n",
    "for bug_no in bugs:\n",
    "    bug = bug_dic[bug_no]\n",
    "    bug.sort()\n",
    "    start_time = int(datetime.datetime.fromtimestamp(bug[0] // 86400 * 86400).timestamp())\n",
    "    end_time = int(datetime.datetime.fromtimestamp((bug[-1] // 86400 + 1) * 86400).timestamp())\n",
    "    data_stream = stream_generation(bug, start_time, end_time)\n",
    "    if bug_no in change_info:\n",
    "        report_info[bug_no] = []\n",
    "        time_serious = change_info[bug_no]\n",
    "        time_len = len(time_serious)\n",
    "        for i in range(0, time_len-2):\n",
    "            start = start_time + time_serious[i] * 3600\n",
    "            change = start_time + time_serious[i+1] * 3600\n",
    "            end = start_time + time_serious[i+2] * 3600\n",
    "            \n",
    "            pre_change = []\n",
    "            post_change = []\n",
    "            doc1 = []\n",
    "            doc2 = []\n",
    "            \n",
    "            for words_list in bugs_dict[bug_no]:\n",
    "                try:\n",
    "                    if int(words_list[0]) >= start and int(words_list[0]) <= change:\n",
    "                        pre_change.append(words_list[1:])\n",
    "                        doc1 += words_list[1:]\n",
    "                    elif int(words_list[0]) >= change and int(words_list[0]) <= end:\n",
    "                        post_change.append(words_list[1:])\n",
    "                        doc2 += words_list[1:]\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            pre_len = len(pre_change)\n",
    "            post_len = len(post_change)\n",
    "            if pre_len > 1 and post_len > 1:\n",
    "                total += 1\n",
    "                try:\n",
    "                    document1 = \" \".join(doc1)\n",
    "                    document2 = \" \".join(doc2)\n",
    "                    inter_cos, extra = cosine_sim(pre_change, post_change, document1, document2, pretrain_model4)\n",
    "                    \n",
    "                    document1 = doc_generate(pre_change[:len(pre_change)//2])\n",
    "                    document2 = doc_generate(pre_change[len(pre_change)//2:])\n",
    "                    pre_cos, pre_extra = cosine_sim(pre_change[:len(pre_change)//2], pre_change[len(pre_change)//2:], document1, document2, pretrain_model4)\n",
    "                    \n",
    "                    document1 = doc_generate(post_change[:len(post_change)//2])\n",
    "                    document2 = doc_generate(post_change[len(post_change)//2:])\n",
    "                    post_cos, post_extra = cosine_sim(post_change[:len(post_change)//2], post_change[len(post_change)//2:], document1, document2, pretrain_model4)\n",
    "                    \n",
    "                    if inter_cos <= pre_cos and inter_cos <= post_cos:\n",
    "                        counter += 1\n",
    "                        report_info[bug_no].append((start, change, end, pre_len, post_len, pre_cos, post_cos, inter_cos, extra))\n",
    "                        \n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "print(counter/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write outputs into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "output = csv.writer(open('word2vec_tfidf_large.csv', 'w'))\n",
    "output.writerow(['Bug No', 'Start point', 'Change point', 'End point', 'prechange comments', 'postchange comments', 'pre cosine distance', 'post cosine distance', 'inter cosine distance', 'extra words'])\n",
    "\n",
    "for key, value in report_info.items():\n",
    "    for info in value:\n",
    "        try:\n",
    "            #if info[5] <= info[6] and info[5] < info[7]:\n",
    "            output.writerow([key, info[0], info[1], info[2], info[3], info[4], info[5], info[6], info[7], info[8]])\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II -- Average similarity between different bug reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several functions for text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a text with contraction'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "replace_contractions(\"this's a text with contraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mr.nothing/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc):\n",
    "    doc = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', doc, flags = re.MULTILINE)\n",
    "    doc = replace_contractions(doc)\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', ' ', doc, flags = re.MULTILINE)\n",
    "    doc = re.sub(r'\\s+', ' ', doc, flags = re.MULTILINE)\n",
    "    doc = re.sub(r'\\n+', ' ', doc, flags = re.MULTILINE)\n",
    "    doc = doc.strip().lower()\n",
    "    tokenized_words = doc.split()\n",
    "    doc = [word for word in tokenized_words if word not in stop_words]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity calculation only based on Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim2(list1, list2, model):\n",
    "    extra = []\n",
    "    vec1 = np.zeros((100, ), dtype='float32')\n",
    "    counter1 = 0\n",
    "    for sub in list1:\n",
    "        for word in sub:\n",
    "            try:\n",
    "                vec1 = np.add(vec1, model[word])\n",
    "                counter1 += 1\n",
    "            except:\n",
    "                extra.append(word)\n",
    "                continue\n",
    "    try:\n",
    "        vec1 = np.divide(vec1, counter1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    vec2 = np.zeros((100, ), dtype='float32')\n",
    "    counter2 = 0\n",
    "    for sub in list2:\n",
    "        for word in sub:\n",
    "            try:\n",
    "                vec2 = np.add(vec2, model[word])\n",
    "                counter2 += 1\n",
    "            except:\n",
    "                extra.append(word)\n",
    "                continue\n",
    "    try:\n",
    "        vec2 = np.divide(vec2, counter2)\n",
    "    except:pass\n",
    "\n",
    "    extra = {i:extra.count(i) for i in set(extra)}\n",
    "    cos_sim = 1 - spatial.distance.cosine(vec1, vec2)\n",
    "    \n",
    "    return cos_sim, extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average similarity for different individual bug reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rate under the average is 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "selected_bugs =[key for key in bugs_dict.keys()]\n",
    "counter = 0\n",
    "counter2 = 0\n",
    "total = 0\n",
    "threashold = 0.8569841307142506\n",
    "for i in range(len(selected_bugs)-1):\n",
    "    for j in range(i+1,len(selected_bugs)):\n",
    "        counter += 1\n",
    "        cos_sim, extra = cosine_sim2(bugs_dict[selected_bugs[i]], bugs_dict[selected_bugs[j]], pretrain_model)\n",
    "        if cos_sim <= threashold:\n",
    "            counter2 += 1\n",
    "            #print(\"The cosine similarity between\",selected_bugs[i], \"and\", selected_bugs[j], \"is\", cos_sim)\n",
    "        total += cos_sim\n",
    "#print(\"The average similarity among the bugs is\", total/counter)\n",
    "print(\"The rate under the average is\", counter2/counter)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| pre-train model | average similarity | under avg |\n",
    "| --- | --- | --- |\n",
    "| word2vec-google-news-300 | 0.8570 | 0.4167 |\n",
    "| fasttext-wiki-news-subwords-300 | 0.8767 | 0.3478 |\n",
    "| glove-wiki-gigaword-300 | 0.8034 | 0.4529 |\n",
    "| bug_comment_model | 0.7470 | 0.4420 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base line generation from 20 newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "target_names = ['alt.atheism',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.misc']\n",
    "\n",
    "datasets = {}\n",
    "for target in target_names:\n",
    "    training_set = fetch_20newsgroups(subset='train', categories=[target])\n",
    "    datasets[target] = []\n",
    "    data = training_set.data\n",
    "    for sentence in data:\n",
    "        datasets[target].append(preprocessing(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mr.nothing/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/Users/mr.nothing/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rate under the average is 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "counter2 = 0\n",
    "total = 0\n",
    "threashold = 0.9353\n",
    "for i in range(len(target_names)-1):\n",
    "    for j in range(i+1,len(target_names)):\n",
    "        counter += 1\n",
    "        cos_sim, extra = cosine_sim2(datasets[target_names[i]], datasets[target_names[j]], pretrain_model4)\n",
    "        if cos_sim <= threashold:\n",
    "            counter2 += 1\n",
    "            #print(\"The cosine similarity between\",selected_bugs[i], \"and\", selected_bugs[j], \"is\", cos_sim)\n",
    "        total += cos_sim\n",
    "#print(\"The average similarity among the bugs is\", total/counter)\n",
    "print(\"The rate under the average is\", counter2/counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| pre-train model | average similarity | under avg |\n",
    "| --- | --- | --- |\n",
    "| word2vec-google-news-300 | 0.8993 | 0.4761 |\n",
    "| fasttext-wiki-news-subwords-300 | 0.9482 | 0.3333|\n",
    "| glove-wiki-gigaword-300 | 0.8965 | 0.4761 |\n",
    "| bug_comment_model | 0.9353 | 0.3333 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
